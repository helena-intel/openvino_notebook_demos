{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spatial-fifteen",
   "metadata": {},
   "source": [
    "# Open Model Zoo Object Detection Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-campus",
   "metadata": {},
   "source": [
    "This demo showcases Object Detection on Open Model Zoo models with Async API. Async API usage can improve the overall frame-rate of the application, because inference and image preprocessing can occur at the same time.\n",
    "\n",
    "This notebook allows you to select a model and an input video, as well as vary the number of streams, threads and requests for the inference.\n",
    "\n",
    "Note: the notebook allows you to upload your own video. It is recommended to use a short video. If you use a video that is longer than a few minutes, you can adjust the `JUMP_FRAMES` setting to a larger value to increase inference speed. With the default setting every tenth frame is analyzed.\n",
    "\n",
    "Other demo objectives are:\n",
    "\n",
    "* Using video as input with OpenCV\n",
    "* Visualizing the resulting bounding boxes\n",
    "* Comparing results and speed of different Open Model Zoo models\n",
    "\n",
    "Note that the frame rates shown in this demo are an indication and not a true measure of performance of a model. Use the [OpenVINO Benchmark Tool](https://github.com/openvinotoolkit/openvino/tree/master/inference-engine/tools/benchmark_tool) to get a better measure of performance.\n",
    "\n",
    "This demo works with a variety of [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) models and allows you to use your own video. [IPywidgets](https://github.com/jupyter-widgets/ipywidgets) widgets are used to easily select a demo video or choose a video from your PC.\n",
    "\n",
    "**Use your own video**\n",
    "\n",
    "Sample videos are downloaded from [Intel IoT Sample Videos](https://github.com/intel-iot-devkit/sample-videos). The link shows previews of the videos. You can optionally choose to upload your own video. If you do so, it is recommended to use a short video. To upload your own video, copy or upload an *.avi* or *.mp4* file to the *videos* subdirectory. This subdirectory will be created automatically if you run this notebook for the first time. You can also create it manually, as a subdirectory of the directory that contains this notebook. You can either copy the files to this directory manually, or use Jupyter Notebook's or Jupyter Lab's functionality to upload files. See this [short video](https://www.youtube.com/watch?v=1bd2QHqQSH4) on how to upload files in Jupyter Lab. After uploading the video, it will appear in the video selection widget. To update this widget with a newly uploaded video, either restart the notebook (Kernel menu->*Restart Kernel and Run All Cells*) or run the *Create widget* cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-stocks",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-platform",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import glob\n",
    "import json\n",
    "import os.path\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, clear_output\n",
    "from ipywidgets import Layout, interact_manual\n",
    "from openvino.inference_engine import IECore, get_version\n",
    "\n",
    "from detection_utils import ColorPalette, download_video, get_model, put_highlighted_text\n",
    "from notebook_utils import AsyncPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-component",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Set the file and directory paths. The default settings expect that the models are located in `open_model_zoo_models` in your `$HOME` directory, typically `C:\\Users\\username` or `/home/username`. You can change this by setting the `base_model_dir` variable to another directory. Set `models_file` to `models-all.lst` to use all supported models, instead of a subset. This wil increase model download and conversion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-exhibit",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## File settings\n",
    "# Directory that contains the Open Model Zoo models. It has subdirectories \"intel\" and \"public\"\n",
    "base_model_dir = os.path.expanduser(\"~/open_model_zoo_models\")\n",
    "# Directory for Open Model Zoo cache files. Caching speeds up subsequent downloads.\n",
    "omz_cache_dir = os.path.expanduser(\"~/open_model_zoo_cache\")\n",
    "models_file = \"models.lst\"  # models-subset.lst contains a subset of supported models.\n",
    "# models_file = 'models.lst'  # models.lst contains all supported models.\n",
    "video_directory = \"videos\"\n",
    "\n",
    "## Model settings\n",
    "DEVICE = \"CPU\"\n",
    "PRECISION = \"FP16\"\n",
    "\n",
    "## Demo settings\n",
    "DOWNLOAD_MODELS = True  # Use Model Downloader to download models from Open Model Zoo\n",
    "JUMP_FRAMES = 60  # Read every n-th frame of the input video\n",
    "PROB_THRESHOLD = 0.5  # The probability threshold for detection predictions\n",
    "DEFAULT_NUM_THREADS = 3  # Default number of threads to use for inference\n",
    "DEFAULT_NUM_STREAMS = 3  # Default number of streams to use for inference\n",
    "DEFAULT_NUM_REQUESTS = 4  # Default maximum number of requests to use for inference\n",
    "\n",
    "## Visualization settings\n",
    "PALETTE = ColorPalette(100)\n",
    "FONT_SCALE = 1\n",
    "THICKNESS = 2\n",
    "\n",
    "# The settings below are only required if you want to use the Model Converter to convert models to OpenVINO IR format.\n",
    "# You can use this demo with models that are already downloaded in IR format, so use of the model optimizer is optional.\n",
    "\n",
    "# The path to the Model Optimizer is required if models need to be converted to IR. The paths below should work for default installations of\n",
    "# the Intel Distribution of OpenVINO Toolkit https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html\n",
    "# Adjust them if you installed OpenVINO in a different location.\n",
    "# Note that you also need to install the Model Optimizer prerequisites. See the documentation for your OS at\n",
    "# https://docs.openvinotoolkit.org/latest/installation_guides.html\n",
    "\n",
    "CONVERT_MODELS = False  # Set to True to use public Open Model Zoo models and convert them with the Model Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-dubai",
   "metadata": {},
   "source": [
    "## Download Models and convert them to IR format\n",
    "\n",
    "The [Model Downloader](https://github.com/openvinotoolkit/open_model_zoo/blob/master/tools/downloader/README.md) downloads models from the Open Model Zoo. Models that are not in OpenVINO IR format are converted to this format by the Model Converter. \n",
    "\n",
    "A subset of [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) models that are compatible with this demo are listed in the file `models_file` (default \"models.lst\") in the same folder as this notebook. By default these models are downloaded, with the `--list=models_file` argument for the Model Downloader. You can choose to download a specific model by using `--name=model_name` instead of `--list=models.lst`. \n",
    "\n",
    "Note that [Model Downloader](https://github.com/openvinotoolkit/open_model_zoo/blob/master/tools/downloader/README.md) and Model Converter are standalone tools that can be executed using the command line. Models have to be downloaded only once. If you already have downloaded the Open Zoo Models, you can set the `base_model_dir` variable in the *Settings* cell to the folder that contains your models (this should be a folder with subfolders `intel` and `public`) and set `DOWNLOAD_MODELS` to `False`.\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\"><i>\n",
    "<b>Note: </b>It will take a while to download and convert all the models. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-respondent",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DOWNLOAD_MODELS:\n",
    "    download_result = ! omz_downloader --output_dir $base_model_dir --cache_dir $omz_cache_dir --precision $PRECISION --list $models_file --jobs 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-relay",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the models that are not in IR format to IR.\n",
    "# This may take a long time!\n",
    "if CONVERT_MODELS:\n",
    "    convert_result = ! omz_converter --download_dir $base_model_dir --list $models_file --precisions $PRECISION --jobs 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-reward",
   "metadata": {},
   "source": [
    "### Get model info\n",
    "\n",
    "The Info Dumper returns information for the Open Model Zoo models. It returns a list of dictionaries with the model name, description, framework, license url, precisions, task type, and the subdirectory for the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-brisbane",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_result = ! omz_info_dumper --list $models_file\n",
    "info = json.loads(info_result.get_nlstr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-humor",
   "metadata": {},
   "source": [
    "Make a list of models that will be shown as options. By default, only models that are already in IR format are shown. Change this by uncommenting the second line to use all models. You need to make sure that the models are in IR format. This can be done by setting `CONVERT_MODELS` to `True` and running all cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-church",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = [model[\"name\"] for model in info if model[\"framework\"] == \"dldt\"]\n",
    "# model_names = [model[\"name\"] for model in info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-heavy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show an example of the information that the Info Dumper returns\n",
    "info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-aging",
   "metadata": {},
   "source": [
    "The `models_file` file, by default \"models.lst\", lists models that are supported by this demo, sorted by architecture. The model names can contain wildcard. For example, `face-detection-????` means that the demo supports all models with a name that starts with `face-detection-` followed by four digits. \n",
    "\n",
    "We create a `model_architectures` dictionary that maps the model names given by the Info Dumper, to an architecture given by `models_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-former",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_architectures = {}\n",
    "with open(models_file) as f:\n",
    "    modellist = f.read().splitlines()\n",
    "\n",
    "for line in modellist[1:]:\n",
    "    if line.startswith(\"# For\"):\n",
    "        _, architecture = line.split(\"=\")\n",
    "    else:\n",
    "        model_architectures[line] = architecture\n",
    "        for modelname in model_names:\n",
    "            if fnmatch.fnmatch(modelname, line):\n",
    "                model_architectures[modelname] = architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-coach",
   "metadata": {},
   "source": [
    "## Create inference functions\n",
    "\n",
    "Create functions that perform the inference, and display the results. \n",
    "\n",
    "`draw_detections` is a function that draws detection boxes found by the network on the video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-tutorial",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_detections(frame, detections, palette, labels, threshold, draw_landmarks=False):\n",
    "    \"\"\"\n",
    "    Draw detection boxes on `frame`.\n",
    "    \"\"\"\n",
    "    size = frame.shape[:2]\n",
    "    for detection in detections:\n",
    "        if detection.score > threshold:\n",
    "            xmin = max(int(detection.xmin), 0)\n",
    "            ymin = max(int(detection.ymin), 0)\n",
    "            xmax = min(int(detection.xmax), size[1])\n",
    "            ymax = min(int(detection.ymax), size[0])\n",
    "            class_id = int(detection.id)\n",
    "            color = palette[class_id]\n",
    "            det_label = labels[class_id] if labels and len(labels) >= class_id else \"#{}\".format(class_id)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                \"{} {:.1%}\".format(det_label, detection.score),\n",
    "                (xmin, ymin - 7),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                0.6,\n",
    "                color,\n",
    "                1,\n",
    "            )\n",
    "            if draw_landmarks:\n",
    "                for landmark in detection.landmarks:\n",
    "                    cv2.circle(frame, landmark, 2, (0, 255, 255), 2)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-broad",
   "metadata": {},
   "source": [
    " The `do_inference_on_video` function performs the inference of a model on a specific video. The helper function `process_results` add the time to the result from the pipeline, so that the inference speed can be computed. The function opens the video file given by `input_filename` with OpenCV's `VideoCapture`. It reads the frames sequentially, `jump_frames` frames at a time. If `jump_frames=1` all frames will be read. By default `jump_frames=10` which means that every tenth frame will be read. While there are new frames, the code:\n",
    "\n",
    "* Checks if there are results from the pipeline. If there are, it records the time, and adds the result to the list of results\n",
    "* Checks if the pipeline is ready. If it is, it sees if there is a new frame. \n",
    "  * If there is a new frame (we have not reached the end of the video), the frame is read, and sent to the detector pipeline for inference. \n",
    "  * If there are no more frames, the video is closed\n",
    "\n",
    "At the end of the function, we wait until the detector is finished, and add the final results to the list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-commander",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_inference_on_video(detector_pipeline, input_filename, jump_frames):\n",
    "    \"\"\"\n",
    "    Perform asynchronous inference on a given detector_pipeline with video from input_filename.\n",
    "    `jump_frames` determines how many frames will be read from the video. If jump_frames=N, every Nth frame\n",
    "    of the video will be read.\n",
    "    \"\"\"\n",
    "    resultlist = []\n",
    "    next_frame_id_to_show = 0\n",
    "    overall_start_time = perf_counter()\n",
    "    sequential_frame_nr = 0\n",
    "\n",
    "    def process_results(results):\n",
    "        \"\"\"Helper function to add inference time to results\"\"\"\n",
    "        outputs, meta = results\n",
    "        meta[\"end_time\"] = perf_counter()\n",
    "        meta[\"overall_start_time\"] = overall_start_time\n",
    "        return outputs, meta\n",
    "\n",
    "    cap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        if detector_pipeline.callback_exceptions:\n",
    "            raise detector_pipeline.callback_exceptions[0]\n",
    "\n",
    "        # Process all completed requests\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            resultlist.append(process_results(results))\n",
    "            next_frame_id_to_show += jump_frames\n",
    "\n",
    "        if detector_pipeline.is_ready():\n",
    "            # Get new image/frame\n",
    "            start_time = perf_counter()\n",
    "            ret, frame = cap.read()\n",
    "            sequential_frame_nr += 1\n",
    "            if not ret:\n",
    "                cap.release()\n",
    "                continue\n",
    "\n",
    "            # Submit for inference\n",
    "            if sequential_frame_nr % jump_frames == 0:\n",
    "                detector_pipeline.submit_data(frame, sequential_frame_nr, {\"frame\": frame, \"start_time\": start_time})\n",
    "\n",
    "        else:\n",
    "            # Wait for empty request\n",
    "            detector_pipeline.await_any()\n",
    "\n",
    "    detector_pipeline.await_all()\n",
    "\n",
    "    while detector_pipeline.has_completed_request():\n",
    "        next_frame_id_to_show = next(iter(detector_pipeline.completed_request_results.keys()))\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            resultlist.append(process_results(results))\n",
    "\n",
    "    return resultlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-timer",
   "metadata": {},
   "source": [
    "The `make_result_frames` function takes the output of the `do_inference_on_video` function and returns a list of videoframes with detection boxes drawn on the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-scope",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_result_frames(inference_result, has_landmarks):\n",
    "    \"\"\" \"\n",
    "    Draw boxes on frames from inference results and return the list of frames.\n",
    "    \"\"\"\n",
    "    framelist = []\n",
    "\n",
    "    for i, (objects, meta) in enumerate(inference_result):\n",
    "        start_time = meta[\"start_time\"]\n",
    "        overall_start_time = meta[\"overall_start_time\"]\n",
    "        end_time = meta[\"end_time\"]\n",
    "        latency = (end_time - start_time) * 1000\n",
    "        fps = (i + 1) / (end_time - overall_start_time)\n",
    "\n",
    "        frame = meta[\"frame\"]\n",
    "        frame = draw_detections(\n",
    "            frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),\n",
    "            detections=objects,\n",
    "            palette=PALETTE,\n",
    "            labels=None,\n",
    "            threshold=PROB_THRESHOLD,\n",
    "            draw_landmarks=has_landmarks,\n",
    "        )\n",
    "        put_highlighted_text(\n",
    "            frame=frame,\n",
    "            message=\"Latency: {:.1f} ms\".format(latency),\n",
    "            position=(20, 30),\n",
    "            font_face=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            font_scale=FONT_SCALE,\n",
    "            color=PALETTE[0],\n",
    "            thickness=THICKNESS,\n",
    "        )\n",
    "        put_highlighted_text(\n",
    "            frame=frame,\n",
    "            message=\"FPS: {:.1f}\".format(fps),\n",
    "            position=(20, 60),\n",
    "            font_face=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            font_scale=FONT_SCALE,\n",
    "            color=PALETTE[0],\n",
    "            thickness=THICKNESS,\n",
    "        )\n",
    "\n",
    "        framelist.append(frame)\n",
    "    return framelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-service",
   "metadata": {},
   "source": [
    "`get_results_for_model` ties everything together. It creates a pipeline for the specified model, runs inference, and creates a numpy array with results. It returns the resulting array and the FPS for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-arkansas",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_results_for_model(modelname, input_filename, num_threads, num_streams, num_requests):\n",
    "    \"\"\"\n",
    "    Creates a pipeline for the specified model, runs inference, and creates a numpy array with results.\n",
    "    The function uses the `info` and `model_architectures` dictionaries that are created in previous notebook cells.\n",
    "\n",
    "    :param modelname: name of the model to use for inference, as given by Info Dumper\n",
    "    :param input_filename: input filename for video to run inference on\n",
    "    :param num_threads: number of threads for inference\n",
    "    :param num_streams: number of streams for inference\n",
    "    :param num_requests: maximum number of requests for inference\n",
    "    :return: list of frames with drawn detection results and the inference FPS\n",
    "    \"\"\"\n",
    "    # Create IE model\n",
    "    model_info = [item for item in info if item[\"name\"] == modelname][0]\n",
    "    model_xml = os.path.join(base_model_dir, model_info[\"subdirectory\"], PRECISION, modelname + \".xml\")\n",
    "    architecture_type = model_architectures[modelname]\n",
    "    ie = IECore()\n",
    "    model = get_model(ie=ie, model=Path(model_xml), architecture_type=architecture_type, labels=None)\n",
    "\n",
    "    # Create Async pipeline\n",
    "    plugin_config = {\n",
    "        \"CPU_THREADS_NUM\": f\"{num_threads}\",\n",
    "        \"CPU_THROUGHPUT_STREAMS\": f\"{num_streams}\",\n",
    "    }\n",
    "    detector_pipeline = AsyncPipeline(ie, model, plugin_config, device=DEVICE, max_num_requests=num_requests)\n",
    "\n",
    "    # Do inference\n",
    "    print(\n",
    "        f\"Starting inference. Model: {modelname}, video: {input_filename},  threads: {num_threads}, streams: {num_streams}, max_num_requests: {num_requests}\"\n",
    "    )\n",
    "    start_time = perf_counter()\n",
    "    inference_result = do_inference_on_video(\n",
    "        detector_pipeline=detector_pipeline, input_filename=input_filename, jump_frames=JUMP_FRAMES\n",
    "    )\n",
    "    end_time = perf_counter()\n",
    "\n",
    "    # Draw inference results on video frames and compute FPS\n",
    "    has_landmarks = architecture_type == \"retina\"\n",
    "    result_frames = make_result_frames(inference_result=inference_result, has_landmarks=has_landmarks)\n",
    "    fps = len(result_frames) / (end_time - start_time)\n",
    "    return result_frames, fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-renewal",
   "metadata": {},
   "source": [
    "## Create widget for selecting videos\n",
    "\n",
    "Create a dropdown widget to select either a sample video that will be downloaded if needed, or an uploaded video. See the top of this notebook for information on how to upload a video of your own. Run this cell after uploading a video, to refresh the widget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-finding",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_video_base_url = \"https://github.com/intel-iot-devkit/sample-videos/raw/master\"\n",
    "with open(\"sample_videos.lst\") as f:\n",
    "    sample_video_filenames = f.read().splitlines()\n",
    "all_videos = glob.glob(os.path.join(video_directory, \"*.mp4\")) + glob.glob(os.path.join(video_directory, \"*.avi\"))\n",
    "own_video_filenames = [\n",
    "    os.path.basename(video) for video in all_videos if os.path.basename(video) not in sample_video_filenames\n",
    "]\n",
    "\n",
    "video_list = [(fn[:-4], fn) for fn in own_video_filenames]\n",
    "video_list += [(f\"sample: {fn[:-4]}\", f\"{sample_video_base_url}/{fn}\") for fn in sample_video_filenames]\n",
    "\n",
    "default_video = [item for item in video_list if item[0] == \"sample: face-demographics-walking-and-pause\"][0]\n",
    "default_index = video_list.index(default_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-binding",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_index = (\n",
    "    0 if len(own_video_filenames) > 0 else default_index\n",
    ")  # If we uploaded a video of our own, use this video by default, otherwise use the face demographics video\n",
    "video = widgets.Dropdown(options=video_list, index=video_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-coupon",
   "metadata": {},
   "source": [
    "## Detection results of one model, drawn on video\n",
    "\n",
    "Select a model and set the number of threads, streams and the maximum number of requests. ipywidgets\\* is used to make widgets for the model and option selection. If a sample video is selected for the first time, it is downloaded to the videos directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-westminster",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "interact_inference = interact_manual.options(manual_name=\"Do inference\")\n",
    "\n",
    "\n",
    "@interact_inference(num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10))\n",
    "def show_results_on_model(\n",
    "    model=model_names,\n",
    "    video=video,\n",
    "    num_threads=DEFAULT_NUM_THREADS,\n",
    "    num_streams=DEFAULT_NUM_STREAMS,\n",
    "    num_requests=DEFAULT_NUM_REQUESTS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform inference and display results for the selected model, with specified number of threads, streams and max number of requests.\n",
    "    NOTE: there is no error checking. Make sure that the selected model exists in IR format.\n",
    "    \"\"\"\n",
    "    input_filename = os.path.join(video_directory, os.path.basename(video))\n",
    "    if not os.path.exists(input_filename):\n",
    "        download_video(video, video_directory)\n",
    "    resultvideo, fps = get_results_for_model(model, input_filename, num_threads, num_streams, num_requests)\n",
    "\n",
    "    for item in resultvideo:\n",
    "        try:\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(0.5)\n",
    "            plt.imshow(item)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    print(\n",
    "        f\"Finished inference. Model: {model},  threads: {num_threads}, streams: {num_streams}, max_num_requests: {num_requests}. FPS: {fps:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-tourism",
   "metadata": {},
   "source": [
    "## Detection results of multiple models\n",
    "\n",
    "Perform inference on selected models and show results on three random frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a widget to select multiple models. By default three face detection models are selected.\n",
    "select_model_widget = widgets.SelectMultiple(\n",
    "    description=\"Models\",\n",
    "    options=model_names,\n",
    "    index=[2, 5, 7],\n",
    "    layout=Layout(display=\"flex\", flex_flow=\"column\"),\n",
    "    disabled=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_inference(modelnames=select_model_widget, num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10))\n",
    "def show_inference_multiple_models(\n",
    "    modelnames,\n",
    "    video=video,\n",
    "    num_threads=DEFAULT_NUM_THREADS,\n",
    "    num_streams=DEFAULT_NUM_STREAMS,\n",
    "    num_requests=DEFAULT_NUM_REQUESTS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform inference for selected models and show results on three random frames of the input video.\n",
    "    \"\"\"\n",
    "    inference_results_multiple_models = []\n",
    "    input_filename = os.path.join(video_directory, os.path.basename(video))\n",
    "    if not os.path.exists(input_filename):\n",
    "        download_video(video, video_directory)\n",
    "\n",
    "    for i, modelname in enumerate(modelnames):\n",
    "        resultvideo, fps = get_results_for_model(modelname, input_filename, num_threads, num_streams, num_requests)\n",
    "        inference_results_multiple_models.append(resultvideo)\n",
    "        print(f\"--- Finished: FPS: {fps:.2f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(3, len(inference_results_multiple_models), figsize=(18, 12), squeeze=False)\n",
    "    indices = random.choices(range(len(inference_results_multiple_models[0])), k=3)\n",
    "    for i, resultvideo in enumerate(inference_results_multiple_models):\n",
    "        modelname = select_model_widget.value[i]\n",
    "        ax[0, i].set_title(modelname)\n",
    "        for j, framenr in enumerate(indices):\n",
    "            ax[j, i].imshow(resultvideo[framenr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
