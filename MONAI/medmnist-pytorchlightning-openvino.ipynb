{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cfd4d-35b9-4e4b-9b44-b417bd4e13b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade git+https://github.com/MedMNIST/MedMNIST.git pytorch_lightning openvino-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c1ee4-7d83-4607-bd5c-3980d8ab46cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## full dataset\n",
    "# https://scholar.cu.edu.eg/?q=afahmy/pages/dataset\n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6906728/\n",
    "# http://arxiv-export-lb.library.cornell.edu/pdf/2110.14795"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5d50d-acad-44d7-a7b2-5d5138c6fe8c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db1c72-8d54-4192-8ffa-ee2ebab67c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import dateutil\n",
    "import matplotlib.pyplot as plt\n",
    "import medmnist\n",
    "import monai\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from addict import Dict\n",
    "from compression.api import DataLoader, Metric\n",
    "from compression.engines.ie_engine import IEEngine\n",
    "from compression.graph import load_model, save_model\n",
    "from compression.graph.model_utils import compress_model_weights\n",
    "from compression.pipeline.initializer import create_pipeline\n",
    "from monai.data import Dataset\n",
    "from monai.networks.nets import DenseNet\n",
    "from monai.transforms import AddChannel, Compose, EnsureType, ToTensor\n",
    "from openvino.inference_engine import IECore\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f489894-b633-4d9c-8f24-2478e380a2fa",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Monai Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247110f4-9b80-4fdb-bccc-2448e4d0e833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MonaiModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self._model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2, init_features=4, growth_rate=4)\n",
    "        self._model = DenseNet(\n",
    "            spatial_dims=2, in_channels=1, out_channels=1, block_config=[4, 8, 6]\n",
    "        ).cpu()\n",
    "\n",
    "        # https://docs.monai.io/en/latest/highlights.html?deterministic-training-for-reproducibility\n",
    "        monai.utils.set_determinism(seed=2.71828, additional_settings=None)\n",
    "\n",
    "        self.loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "        self.metric = monai.metrics.ConfusionMatrixMetric(metric_name=\"accuracy\")\n",
    "        self.best_val_accuracy = 0\n",
    "        self.best_val_epoch = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters())\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        labels = labels.float()\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        self.log(\"train_loss\", loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        labels = labels.float()\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels)\n",
    "\n",
    "        # Compute statistics for metric computation\n",
    "        y_true = labels.long()\n",
    "        y_pred = torch.sigmoid(output).round().long()\n",
    "        self.metric(y_pred, y_true)\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return {\"val_loss\": loss, \"val_number\": len(output)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss, num_items = 0, 0\n",
    "\n",
    "        for output in outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "        # mean_val_dice = self.metric.avg_value[\"F1\"]\n",
    "        # self.metric.reset()\n",
    "        mean_val_accuracy = self.metric.aggregate()[0].item()\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        self.logger.experiment.add_scalar(\"Loss/Validation\", mean_val_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\n",
    "            \"Accuracy/Validation\", mean_val_accuracy, self.current_epoch\n",
    "        )\n",
    "        self.log(\"accuracy\", mean_val_accuracy, prog_bar=True, logger=False)\n",
    "\n",
    "        if mean_val_accuracy > self.best_val_accuracy:\n",
    "            self.best_val_accuracy = mean_val_accuracy\n",
    "            self.best_val_epoch = self.current_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f3a6b-304e-494c-b46d-657b6f19c21d",
   "metadata": {},
   "source": [
    "## PyTorch Lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e9fa6-3ec8-47b4-9c52-b36361e02753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        random.seed(1.414213)\n",
    "        transforms = Compose([ToTensor(dtype=torch.float), AddChannel(), EnsureType()])\n",
    "\n",
    "        train_data = medmnist.BreastMNIST(split=\"train\", transform=None, download=True)\n",
    "        val_data = medmnist.BreastMNIST(split=\"val\", transform=None, download=True)\n",
    "\n",
    "        self.dataset_train = Dataset(\n",
    "            [(np.array(item[0]), item[1][0]) for item in train_data], transform=transforms\n",
    "        )\n",
    "        self.dataset_val = Dataset(\n",
    "            [(np.array(item[0]), item[1][0]) for item in val_data], transform=transforms\n",
    "        )\n",
    "\n",
    "        print(f\"Setup train dataset: {len(self.dataset_train)} items\")\n",
    "        print(f\"Setup val dataset: {len(self.dataset_val)} items\")\n",
    "\n",
    "        assert len(self.dataset_train) > 0, \"Train dataset is empty.\"\n",
    "        assert len(self.dataset_val) > 0, \"Val dataset is empty\"\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f40163-6795-445d-b930-81353afeb416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir tb_logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f7f9c-ea9c-48cb-b3c9-a172caa33736",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e34c28-079c-4bff-8eed-242e49a9b67a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=24)\n",
    "model = MonaiModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11531a75-e879-4756-97e6-30801bcf959f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"medmnist_breast\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"accuracy\", mode=\"max\", save_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157ce29-7bdd-4422-928e-3fdecfad3f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.setup()\n",
    "input_image, input_label = next(iter(data.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a2ee8-340e-4692-9a24-07c655d669d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    gpus=1 if USE_CUDA else 0,\n",
    "    logger=logger,\n",
    "    precision=16 if USE_CUDA else 32,\n",
    "    limit_train_batches=0.5,\n",
    "    limit_val_batches=0.5,\n",
    "    # callbacks=[checkpoint_callback],\n",
    "    fast_dev_run=False,  # set to True to quickly test Lightning model\n",
    ")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print(start.strftime(\"%H:%M:%S\"))\n",
    "try:\n",
    "    trainer.fit(model, data)\n",
    "finally:\n",
    "    end = datetime.datetime.now()\n",
    "    print(end.strftime(\"%H:%M:%S\"))\n",
    "    delta = dateutil.relativedelta.relativedelta(end, start)\n",
    "    print(f\"Training duration: {delta.hours:02d}:{delta.minutes:02d}:{delta.seconds:02d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98752a-4583-4f51-a35a-f4f207982cbb",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79260a-9cd4-476b-a5b4-7f6550874a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_path = \"medmnist_breast.onnx\"\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "torch.onnx.export(model._model.cpu().eval(), dummy_input, onnx_path, opset_version=10)\n",
    "print(f\"Exported ONNX model to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a529f0-932c-4634-bff0-0081cd6b859f",
   "metadata": {},
   "source": [
    "## Convert to OpenVINO IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562e42e-d6d8-4e7b-9af5-fcae1cf09fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mo --input_model medmnist_breast.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951cdc55-d015-4b2b-83d3-f7f6a9e8121d",
   "metadata": {},
   "source": [
    "## POT\n",
    "\n",
    "### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b2631-1af7-4c29-b263-fab63306675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid function is used to transform the result of the network\n",
    "# to binary segmentation masks\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._name = \"accuracy\"\n",
    "        self._matches = []\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"Returns accuracy metric value for the last model output.\"\"\"\n",
    "        return {self._name: [self._matches[-1]]}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\"\n",
    "        Returns accuracy metric value for all model outputs. Results per image are stored in\n",
    "        self._matches, where True means a correct prediction and False a wrong prediction.\n",
    "        Accuracy is computed as the number of correct predictions divided by the total\n",
    "        number of predictions.\n",
    "        \"\"\"\n",
    "        num_correct = np.count_nonzero(self._matches)\n",
    "        return {self._name: num_correct / len(self._matches)}\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\"Updates prediction matches.\n",
    "\n",
    "        :param output: model output\n",
    "        :param target: annotations\n",
    "        \"\"\"\n",
    "        predict = sigmoid(output[0]).round().astype(np.uint8).squeeze()\n",
    "        match = predict == target[0].squeeze().numpy().astype(np.uint8)\n",
    "        # print(predict)\n",
    "        # print(target[0].squeeze().item())\n",
    "        # print('...')\n",
    "        self._matches.append(match)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the Accuracy metric. This is a required method that should initialize all\n",
    "        attributes to their initial value.\n",
    "        \"\"\"\n",
    "        self._matches = []\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
    "                             'type': metric type\n",
    "        \"\"\"\n",
    "        return {self._name: {\"direction\": \"higher-better\", \"type\": \"accuracy\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7d826-0a24-46b5-b3df-62bc2d966a10",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb35056-8d63-4d8f-b1e2-deac0ac749c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    DataLoader for image data that is stored in a directory per category. For example, for\n",
    "    categories _rose_ and _daisy_, rose images are expected in data_source/rose, daisy images\n",
    "    in data_source/daisy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        :param dataset: dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of elements in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get item from self.dataset at the specified index.\n",
    "        Returns (annotation, image), where annotation is a tuple (index, class_index)\n",
    "        and image a preprocessed image in network shape\n",
    "        \"\"\"\n",
    "        image, label = self.dataset[index]\n",
    "        annotation = (index, label)\n",
    "        return annotation, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb5109-9a51-45d5-b4e2-b1c04ab57caf",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771da177-28d7-47da-82db-c39fd90abc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = Dict(\n",
    "    {\n",
    "        \"model_name\": \"medmnist_breast\",\n",
    "        \"model\": \"medmnist_breast.xml\",\n",
    "        \"weights\": \"medmnist_breast.bin\",\n",
    "    }\n",
    ")\n",
    "\n",
    "engine_config = Dict({\"device\": \"CPU\", \"stat_requests_number\": 2, \"eval_requests_number\": 2})\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        \"name\": \"AccuracyAwareQuantization\",\n",
    "        \"params\": {\n",
    "            \"target_device\": \"CPU\",\n",
    "            \"preset\": \"mixed\",\n",
    "            \"stat_subset_size\": 1000,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ab1a1-c47a-4cf5-8a0e-0f7dd6f975a5",
   "metadata": {},
   "source": [
    "### Execute POT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2628db-256d-45be-b72b-0dc6998583f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the model\n",
    "model = load_model(model_config=model_config)\n",
    "original_model = copy.deepcopy(model)\n",
    "\n",
    "# Step 2: Initialize the data loader\n",
    "data_loader = ClassificationDataLoader(dataset=data.dataset_val)\n",
    "\n",
    "# Step 3 (Optional. Required for AccuracyAwareQuantization): Initialize the metric\n",
    "#        Compute metric results on original model\n",
    "metric = Accuracy()\n",
    "\n",
    "# Step 4: Initialize the engine for metric calculation and statistics collection\n",
    "engine = IEEngine(config=engine_config, data_loader=data_loader, metric=metric)\n",
    "\n",
    "# Step 5: Create a pipeline of compression algorithms\n",
    "pipeline = create_pipeline(algo_config=algorithms, engine=engine)\n",
    "\n",
    "# Step 6: Execute the pipeline\n",
    "compressed_model = pipeline.run(model=model)\n",
    "\n",
    "# Step 7 (Optional): Compress model weights quantized precision\n",
    "#                    in order to reduce the size of final .bin file\n",
    "compress_model_weights(model=compressed_model)\n",
    "\n",
    "# Step 8: Save the compressed model and get the path to the model\n",
    "compressed_model_paths = save_model(\n",
    "    model=compressed_model, save_path=os.path.join(os.path.curdir, \"model/optimized\")\n",
    ")\n",
    "\n",
    "compressed_model_xml = Path(compressed_model_paths[0][\"model\"])\n",
    "print(f\"The quantized model is stored in {compressed_model_xml}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de967a9-41c6-4c01-81c6-6a9e15fd3823",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f19b86-f25c-4022-b4bb-6a3c3c781e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 (Optional): Evaluate the original and compressed model. Print the results\n",
    "original_metric_results = pipeline.evaluate(original_model)\n",
    "if original_metric_results:\n",
    "    print(f\"Accuracy of the original model:  {next(iter(original_metric_results.values())):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133322b-5cb1-4512-8409-c01a3c1e7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_metric_results = pipeline.evaluate(compressed_model)\n",
    "if quantized_metric_results:\n",
    "    print(f\"Accuracy of the quantized model: {next(iter(quantized_metric_results.values())):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4983ca-01f1-4867-a297-6f343c65b86c",
   "metadata": {},
   "source": [
    "## Benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca87e49-d3d6-4502-8e4b-af64a06da3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_path: os.PathLike,\n",
    "                    device: str = \"CPU\",\n",
    "                    seconds: int = 60, api: str = \"async\",\n",
    "                    batch: int = 1, \n",
    "                    cache_dir=\"model_cache\"):\n",
    "    ie = IECore()\n",
    "    model_path = Path(model_path)\n",
    "    if (\"GPU\" in device) and (\"GPU\" not in ie.available_devices):\n",
    "        raise ValueError(f\"A GPU device is not available. Available devices are: {ie.available_devices}\")\n",
    "    else:\n",
    "        benchmark_command = f\"benchmark_app -m {model_path} -d {device} -t {seconds} -api {api} -b {batch} -cdir {cache_dir}\"\n",
    "        display(Markdown(f\"**Benchmark {model_path.name} with {device} for {seconds} seconds with {api} inference**\"));\n",
    "        display(Markdown(f\"Benchmark command: `{benchmark_command}`\"));\n",
    "\n",
    "        benchmark_output = %sx $benchmark_command\n",
    "        benchmark_result = [line for line in benchmark_output\n",
    "                            if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "        print(\"\\n\".join(benchmark_result))\n",
    "        print()\n",
    "        if \"MULTI\" in device:\n",
    "            devices = device.replace(\"MULTI:\",\"\").split(\",\")\n",
    "            for single_device in devices:\n",
    "                print(f\"{single_device} device: {ie.get_metric(device_name=single_device, metric_name='FULL_DEVICE_NAME')}\")\n",
    "        else:\n",
    "            print(f\"Device: {ie.get_metric(device_name=device, metric_name='FULL_DEVICE_NAME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24802c7-6614-4516-83ef-df18a86100dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## FP32 model\n",
    "benchmark_model(\"medmnist_breast.xml\", device=\"CPU\", seconds=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafa3cd-9751-42c4-869d-e9560d200a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INT8 model\n",
    "benchmark_model(compressed_model_xml, \"CPU\", seconds=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb0799-fd17-4b4f-a873-31e407741fcc",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ecf5d-44c0-486b-98f7-e1263db88009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "\n",
    "data = DataModule(batch_size=6)\n",
    "data.setup()\n",
    "model = MonaiModel()\n",
    "net = ie.read_network(compressed_model_xml)\n",
    "net.batch_size = 6\n",
    "counts = []\n",
    "exec_net = ie.load_network(net, \"CPU\")\n",
    "input_layer = next(iter(exec_net.input_info))\n",
    "for input_image, input_label in data.val_dataloader():\n",
    "    raw_result = exec_net.infer(inputs={input_layer: input_image})[\"386\"]\n",
    "    result = sigmoid(raw_result).round().astype(np.uint8)\n",
    "    counts.append(input_label.numpy().squeeze() == result.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019d5b8-3798-4360-9c38-959cd65cd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(counts) / np.size(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedda9a-7908-47bf-94b3-b373693aaed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(nrows=4, ncols=6, figsize=(15, 12))\n",
    "# for i, ax in enumerate(axs.ravel()):\n",
    "#     ax.imshow(input_image[i][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958a813-0dfe-4b04-bd19-bab306d47c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
